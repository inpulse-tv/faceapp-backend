{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import librairies","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-22T09:09:28.701976Z","iopub.execute_input":"2022-04-22T09:09:28.702301Z","iopub.status.idle":"2022-04-22T09:09:28.727209Z","shell.execute_reply.started":"2022-04-22T09:09:28.702221Z","shell.execute_reply":"2022-04-22T09:09:28.726507Z"}}},{"cell_type":"code","source":"## import matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport io\nimport cv2\nimport numpy as np\nfrom os import listdir\nfrom os.path import isfile, join\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n#import keras\nimport tensorflow as tf\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications import ResNet50\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPool2D, Input, AveragePooling2D\nfrom skimage.feature import hog\nfrom skimage import data, exposure\n\nimport tensorflow_addons as tfa\nimport random\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-04-22T09:09:30.239964Z","iopub.execute_input":"2022-04-22T09:09:30.240848Z","iopub.status.idle":"2022-04-22T09:09:36.397128Z","shell.execute_reply.started":"2022-04-22T09:09:30.240782Z","shell.execute_reply":"2022-04-22T09:09:36.396312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# example of loading the keras facenet model\nfrom keras.models import load_model\n# load the model\nmodel = load_model('../input/facenet-model/facenet_keras.h5')\n# summarize input and output shape\nprint(model.inputs)\nprint(model.outputs)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T09:11:57.931771Z","iopub.execute_input":"2022-04-19T09:11:57.932225Z","iopub.status.idle":"2022-04-19T09:12:04.533534Z","shell.execute_reply.started":"2022-04-19T09:11:57.932192Z","shell.execute_reply":"2022-04-19T09:12:04.53277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing functions","metadata":{}},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    \"\"\"\n    A class that generate data batches using their paths.\n    It is used when you have a big data-set that does not fit the memory\n    ...\n    \n    Attributes\n    ----------\n    dataset: dictionary\n        Its keys are the labels and its values are the paths of the images \n        of each label.\n    dataset_path: str\n        The path of the data-set\n    shuffle: bool\n        True if we want to shuffle the data and vice-versa\n    batch_size: int\n        The size of the batch\n    no_of_people: int\n        The number of labels (in our case people are the labels)\n        \n    Methods\n    -------\n    curate_dataset(dataset_path)\n        Create the data-set dictionary\n    on_epoch_end()\n        Shuffle the labels if shuffle=True\n    get_image(person, index)\n        Read, resize and pre-process the image (given at 'index' in the label 'person')\n    \"\"\"\n\n    def __init__(self, dataset_path, batch_size=5, shuffle=True):\n        \"\"\"\n        class initialization\n      \n        param: dataset_path: The path of the data-set\n        parma: shuffle: True if we want to shuffle the data and vice-versa \n        param: batch_size: The size of the batch\n        \"\"\"\n\n        self.dataset = self.curate_dataset(dataset_path)\n        self.dataset_path = dataset_path\n        self.shuffle = shuffle\n        self.batch_size =batch_size\n        self.no_of_people = len(list(self.dataset.keys()))\n        self.on_epoch_end()\n        print(self.dataset.keys())\n        \n    def __getitem__(self, index):\n        \"\"\"\n        Generate the batch\n        \n        param: index: the index of the batch\n        \"\"\"\n\n        people = list(self.dataset.keys())[index * self.batch_size: (index + 1) * self.batch_size]\n        P = []\n        A = []\n        N = []\n        \n        for person in people:\n            anchor_index = random.randint(0, len(self.dataset[person])-1)\n            a = self.get_image(person, anchor_index)\n            \n            positive_index = random.randint(0, len(self.dataset[person])-1)\n            while positive_index == anchor_index and len(self.dataset[person]) != 1:\n                positive_index = random.randint(0, len(self.dataset[person])-1)\n            p = self.get_image(person, positive_index)\n            \n            negative_person_index = random.randint(0, self.no_of_people - 1)\n            negative_person = list(self.dataset.keys())[negative_person_index]\n            while negative_person == person:\n                negative_person_index = random.randint(0, self.no_of_people - 1)\n                negative_person = list(self.dataset.keys())[negative_person_index]\n            \n            negative_index = random.randint(0, len(self.dataset[negative_person])-1)\n            n = self.get_image(negative_person, negative_index)\n            \n            P.append(p)\n            A.append(a)\n            N.append(n)\n        \n        A = np.asarray(A).reshape(-1, 100, 100,1)\n        N = np.asarray(N).reshape(-1, 100, 100,1)\n        P = np.asarray(P).reshape(-1, 100, 100,1)\n        #print(A.shape, P.shape, N.shape)\n        return [A, P, N]\n        \n    def __len__(self):\n        return self.no_of_people // self.batch_size\n        \n    def curate_dataset(self, dataset_path):\n        \"\"\"\n        create the data-set dictionary. Its keys are the labels and its values \n        are the paths of the images of each label.\n        \n        param: dataset_path: the path of the data-set\n        \"\"\"\n\n        dataset = {}\n        dirs = [dir for dir in listdir(dataset_path)]\n        for dir in dirs: \n            fichiers = [f for f in listdir(dataset_path+dir) if \"jpeg\" in f or \"png\" in f]\n            for f in fichiers:\n                if dir in dataset.keys():\n                    dataset[dir].append(f)\n                else:\n                    dataset[dir] = [f]\n        return dataset\n    \n    def on_epoch_end(self):\n        \"\"\"\n        shuffle the labels if shuffle=True \n        \"\"\"\n\n        if self.shuffle:\n            keys = list(self.dataset.keys())\n            random.shuffle(keys)\n            dataset_ =  {}\n            for key in keys:\n                dataset_[key] = self.dataset[key]\n            self.dataset = dataset_\n            \n    def get_image(self, person, index):\n        \"\"\"\n        read, resize and pre-process the image\n        \n        param: person: the label (celebrity name)\n        param: index: the image of index \"index\" in the list dataset[person]\n        :return: pre-processed image\n        \"\"\"\n\n        image = cv2.imread(os.path.join(self.dataset_path, os.path.join(person, self.dataset[person][index])))\n        image = np.asarray(image, dtype=np.float32)\n        image = cv2.resize(image, (100, 100), interpolation = cv2.INTER_AREA)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        image = image.astype('float32')\n        mean, std = image.mean(), image.std()\n        image = (image - mean) / std\n        return image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data(dataset_path):\n    \"\"\"\n    Extract images with their labels and names\n    \n    param: dataset_path: the path of the dataset\n    :return: a tuple of images, labels and names in an array format\n    \"\"\"\n    \n    images = []\n    labels = []\n    names = []\n    try:\n        \n        for dire in listdir(dataset_path): \n            for f in listdir(dataset_path+'/'+dire):\n                if \"jpeg\" in f or \"png\" in f or \"PNG\" in f or \"jpg\" in f:\n                    image_path = dataset_path + '/' + dire + '/' + f\n                    image = cv2.imread(image_path)\n                    image = np.asarray(image, dtype=np.float32)\n                    image = cv2.resize(image, (100, 100), interpolation = cv2.INTER_AREA)\n                    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n                    image = image.astype('float32')\n                    mean, std = image.mean(), image.std()\n                    image = (image - mean) / std\n\n                    images.append(image)\n                    labels.append(dire)\n                    names.append(f)\n    except:\n         for f in listdir(dataset_path):\n                if \"jpeg\" in f or \"png\" in f or \"PNG\" in f or \"jpg\" in f:\n                    image_path = dataset_path + '/' + f\n                    image = cv2.imread(image_path)\n                    image = np.asarray(image, dtype=np.float32)\n                    image = cv2.resize(image, (100, 100), interpolation = cv2.INTER_AREA)\n                    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n                    image = image.astype('float32')\n                    mean, std = image.mean(), image.std()\n                    image = (image - mean) / std\n\n                    images.append(image)\n                    labels.append(f)\n                    names.append(f)\n    return np.array(images), np.array(labels), np.array(names)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T09:09:36.398904Z","iopub.execute_input":"2022-04-22T09:09:36.399157Z","iopub.status.idle":"2022-04-22T09:09:36.413411Z","shell.execute_reply.started":"2022-04-22T09:09:36.399123Z","shell.execute_reply":"2022-04-22T09:09:36.41111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy.linalg import norm\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.linear_model import SGDOneClassSVM\nfrom sklearn.covariance import EllipticEnvelope\ndef get_clean_data(labels, embeds, names, threshold=9, method='distance'):\n    \"\"\"\n    This function clean the data-set\n    \n    :param labels: the images labels\n    :param embeds: the images embeddings\n    :param names: the file names of the images\n    :param threshold: according to this threshold we classify each image\n    :param method: the method used to clean the data\n    :return: a tuple of the outliers, cleaned embedding and labels, each one in an array format\n    \"\"\"\n    \n    outliers =  {}\n    clean_embed = []\n    clean_labels = []\n    for k in set(labels):\n        \n        if method == 'distance':\n            filter_ = []\n            center = sum(embeds[k])/len(embeds[k])\n            for e in embeds[k]:\n                filter_.append(norm(e - center))\n            filter_ = np.array(filter_)\n            outliers[k] = np.array(names[k])[np.where(filter_ >= threshold)]\n            clean_embed.extend(np.array(embeds[k])[np.where(filter_ < threshold)])\n            n = len(np.array(embeds[k])[np.where(filter_ < threshold)])\n            \n        elif method == 'Gauss':\n            m = np.mean(embeds[k], axis=0)\n            v = np.var(embeds[k], axis=0)\n            filter_ = multivariateGaussian(embeds[k], m, v)\n            outliers[k] = np.array(names[k])[np.where(filter_ <= threshold)]\n            clean_embed.extend(np.array(embeds[k])[np.where(filter_ > threshold)])\n            n = len(np.array(embeds[k])[np.where(filter_ > threshold)])\n        \n        elif method == 'one_class_svm':\n            filter_ = SGDOneClassSVM(random_state=0).fit_predict(embeds[k])\n            outliers[k] = np.array(names[k])[np.where(filter_ == -1)]\n            clean_embed.extend(np.array(embeds[k])[np.where(filter_ == 1)])\n            n = len(np.array(embeds[k])[np.where(filter_ == 1)])\n        clean_labels.extend([k]*n)\n        #print(filter_, np.array(names[k]))\n\n    return outliers, np.array(clean_embed), np.array(clean_labels)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T09:09:36.414635Z","iopub.execute_input":"2022-04-22T09:09:36.414898Z","iopub.status.idle":"2022-04-22T09:09:36.57678Z","shell.execute_reply.started":"2022-04-22T09:09:36.414861Z","shell.execute_reply":"2022-04-22T09:09:36.575705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_embeddings(model, images):\n    \"\"\"\n    Get the embeddings of the images\n    \n    param: model: the embedding model (FaceNet)\n    :return: an array of embeddings\n    \"\"\"\n    embeddings = []\n    for img in images:\n        samples = np.expand_dims(img, axis=0)\n        yhat = model.predict(samples)\n        embeddings.append(yhat[0]) \n    return np.array(embeddings)    ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T09:09:37.057074Z","iopub.execute_input":"2022-04-22T09:09:37.057372Z","iopub.status.idle":"2022-04-22T09:09:37.064547Z","shell.execute_reply.started":"2022-04-22T09:09:37.057329Z","shell.execute_reply":"2022-04-22T09:09:37.063665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def multivariateGaussian(X, mu, sigma):\n    \"\"\"\n    Compute the mulvariate gaussian distribution probability\n    \n    param: X: the input data\n    param: mu: the mean\n    param: sigma: the standard deviation\n    :return: the probability of X\n    \"\"\"\n    k = len(mu)\n    sigma=np.diag(sigma)\n    X = X - mu.T\n    prob = 1/((2*np.pi)**(k/2)*(np.linalg.det(sigma)**0.5))* np.exp(-0.5* np.sum(X @ np.linalg.pinv(sigma) * X,axis=1))\n    return prob","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Our model","metadata":{}},{"cell_type":"code","source":"#pip install tensorflow_addons","metadata":{"execution":{"iopub.status.busy":"2022-04-13T08:13:15.218234Z","iopub.execute_input":"2022-04-13T08:13:15.218506Z","iopub.status.idle":"2022-04-13T08:13:15.224985Z","shell.execute_reply.started":"2022-04-13T08:13:15.218471Z","shell.execute_reply":"2022-04-13T08:13:15.22423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape= (100, 100, 1)\n\ndef create_encoder(input_shape):\n    \"\"\"\n    Encoder architecture\n    \n    param: input_shape: a tuple that represents the shape of the images\n    :return: the encoder architecture\n    \"\"\"\n    \n    model = Sequential()\n    \n    model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu', input_shape=input_shape, padding = 'Same'))\n    model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu',padding = 'Same'))\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    \n    model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation='relu', padding = 'Same'))\n    model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation='relu', padding = 'Same'))\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    \n    model.add(Conv2D(filters = 128, kernel_size = (3, 3), activation='relu', padding = 'Same'))\n    model.add(Conv2D(filters = 128, kernel_size = (3, 3), activation='relu', padding = 'Same'))\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    \n    model.add(Conv2D(filters = 256, kernel_size = (3, 3), activation='relu', padding = 'Same'))\n    model.add(Conv2D(filters = 256, kernel_size = (3, 3), activation='relu', padding = 'Same'))\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    \n    model.add(Conv2D(filters = 512, kernel_size = (3, 3), activation='relu', padding = 'Same'))\n    model.add(Conv2D(filters = 512, kernel_size = (3, 3), activation='relu', padding = 'Same'))\n    model.add(AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n    model.add(Dropout(0.4))\n    \n    model.add(Flatten())\n    model.add(Dense(256, activation = \"relu\"))\n    return model\n\nencoder = create_encoder((100,100,1))\nencoder.summary()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-22T09:09:41.510732Z","iopub.execute_input":"2022-04-22T09:09:41.511023Z","iopub.status.idle":"2022-04-22T09:09:43.797971Z","shell.execute_reply.started":"2022-04-22T09:09:41.510992Z","shell.execute_reply":"2022-04-22T09:09:43.797275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dropout_rate = 0.5\ninput_shape=(100,100,1)\nhidden_units = 512\n\ndef create_classifier(encoder, trainable=True):\n    \"\"\"\n    Classifier architecture\n    \n    param: encoder : the encoder model\n    param: trainable: True if you want to retrain the encoder, False otherwise\n    :return: model: the whole encoder-classifer model\n    \"\"\"\n    for layer in encoder.layers:\n        layer.trainable = trainable\n\n    inputs = Input(shape=input_shape)\n    features = encoder(inputs)\n    #features = Dropout(dropout_rate)(features)\n    features = Dense(hidden_units, activation=\"relu\")(features)\n    features = Dropout(dropout_rate)(features)\n    features = Dense(hidden_units, activation=\"relu\")(features)\n    features = Dropout(dropout_rate)(features)\n    outputs = Dense(num_classes, activation=\"softmax\")(features)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"our_classifier\")\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(10e-5),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n    )\n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T09:11:44.023496Z","iopub.execute_input":"2022-04-22T09:11:44.023777Z","iopub.status.idle":"2022-04-22T09:11:44.033325Z","shell.execute_reply.started":"2022-04-22T09:11:44.023741Z","shell.execute_reply":"2022-04-22T09:11:44.032561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create classifier\nclassifier = create_classifier(encoder, trainable=True)\nclassifier.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T10:00:11.103011Z","iopub.execute_input":"2022-04-22T10:00:11.103526Z","iopub.status.idle":"2022-04-22T10:00:11.187648Z","shell.execute_reply.started":"2022-04-22T10:00:11.103485Z","shell.execute_reply":"2022-04-22T10:00:11.18666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Contrastive loss","metadata":{}},{"cell_type":"code","source":"import keras\nclass SupervisedContrastiveLoss(keras.losses.Loss):\n    \"\"\"\n    Class of supervised contrastive loss\n    ...\n    \n    Attribute\n    ---------\n    temperature: int\n        Warm-up steps\n    \"\"\"\n    def __init__(self, temperature=1, name=None):\n        \"\"\"\n        Class initialization\n        \n        param: temperature: the warm-up steps\n        \"\"\"\n        super(SupervisedContrastiveLoss, self).__init__(name=name)\n        self.temperature = temperature\n\n    def __call__(self, labels, feature_vectors, sample_weight=None):\n        \"\"\"\n        The call function\n        \n        param: labels: images labels\n        param: feature_vectors: images embeddings\n        :return: the loss function value\n        \"\"\"\n        \n        # Normalize feature vectors\n        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n        # Compute logits\n        logits = tf.divide(\n            tf.matmul(\n                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n            ),\n            self.temperature,\n        )\n        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n\n\n# def add_projection_head(encoder):\n#     inputs = keras.Input(shape=input_shape)\n#     features = encoder(inputs)\n#     outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n#     model = keras.Model(\n#         inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n#     )\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-22T09:11:44.036122Z","iopub.execute_input":"2022-04-22T09:11:44.036528Z","iopub.status.idle":"2022-04-22T09:11:44.046832Z","shell.execute_reply.started":"2022-04-22T09:11:44.036485Z","shell.execute_reply":"2022-04-22T09:11:44.045994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create encoder\ninput_shape= (100, 100, 1)\nencoder = create_encoder(input_shape)\n# encoder_with_projection_head = add_projection_head(encoder)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T09:11:44.048014Z","iopub.execute_input":"2022-04-22T09:11:44.048538Z","iopub.status.idle":"2022-04-22T09:11:44.159836Z","shell.execute_reply.started":"2022-04-22T09:11:44.048483Z","shell.execute_reply":"2022-04-22T09:11:44.159104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get data to train encoder\nimages_train, labels_train, names_train = get_data(\"../input/475-dataset/475_dataset/train/\")\nimages_test, labels_test, names_test = get_data(\"../input/475-dataset/475_dataset/test/\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\n# preprocess data\nle = preprocessing.LabelEncoder()\nle.fit(labels_train)\nlabels_train = le.transform(labels_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_train = images_train.reshape(-1, 100, 100,1)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T09:11:44.176988Z","iopub.execute_input":"2022-04-22T09:11:44.177489Z","iopub.status.idle":"2022-04-22T09:11:44.182149Z","shell.execute_reply.started":"2022-04-22T09:11:44.177451Z","shell.execute_reply":"2022-04-22T09:11:44.181173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# # data augmentation\n# datagen = ImageDataGenerator(\n#     featurewise_center=True,\n#     featurewise_std_normalization=True,\n#     rotation_range=20,\n#     width_shift_range=0.2,\n#     height_shift_range=0.2,\n#     horizontal_flip=True,\n#     validation_split=0.2,\n#     brightness_range = [0.8, 1.2],\n#     zoom_range = [1-0.1, 1+0.1],\n#     fill_mode='constant'\n#         )\n# datagen.fit(images_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T08:51:51.33761Z","iopub.execute_input":"2022-04-14T08:51:51.337869Z","iopub.status.idle":"2022-04-14T08:51:51.367017Z","shell.execute_reply.started":"2022-04-14T08:51:51.337842Z","shell.execute_reply":"2022-04-14T08:51:51.365524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualization\nimage = plt.imread(\"../input/dataset2/dataset/train/Ariana Grande/Ariana Grande1.jpeg\")\niterator =  datagen.flow(np.array([image]), batch_size=9)\nfor j in range(9):\n    plt.subplot(330 + 1 + j)\n    chunk = iterator.next()\n    sub_img = chunk[0].astype('uint8')\n    plt.imshow(sub_img)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compile encoder\nencoder.compile(\n    optimizer=tf.keras.optimizers.Adam(10e-5),\n    loss=SupervisedContrastiveLoss(0.03),\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T09:11:44.575706Z","iopub.execute_input":"2022-04-22T09:11:44.576289Z","iopub.status.idle":"2022-04-22T09:11:44.586694Z","shell.execute_reply.started":"2022-04-22T09:11:44.576243Z","shell.execute_reply":"2022-04-22T09:11:44.585925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get data to train classifier\nimages_train_clf, labels_train_clf, names_train_clf = get_data(\"../input/dataset2/dataset/train/\")\nimages_test_clf, labels_test_clf, names_test_clf = get_data(\"../input/dataset2/dataset/test/\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T09:26:22.244772Z","iopub.execute_input":"2022-04-19T09:26:22.245046Z","iopub.status.idle":"2022-04-19T09:26:23.782421Z","shell.execute_reply.started":"2022-04-19T09:26:22.245016Z","shell.execute_reply":"2022-04-19T09:26:23.781606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make all layers trainable\nfor layer in encoder.layers:\n    layer.trainable = True","metadata":{"execution":{"iopub.status.busy":"2022-04-19T09:57:31.917699Z","iopub.execute_input":"2022-04-19T09:57:31.918151Z","iopub.status.idle":"2022-04-19T09:57:31.924273Z","shell.execute_reply.started":"2022-04-19T09:57:31.918099Z","shell.execute_reply":"2022-04-19T09:57:31.923513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit encoder\nencoder.fit(images_train, labels_train, batch_size=30, epochs=600)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T09:58:26.103298Z","iopub.execute_input":"2022-04-19T09:58:26.103576Z","iopub.status.idle":"2022-04-19T10:02:48.666096Z","shell.execute_reply.started":"2022-04-19T09:58:26.103546Z","shell.execute_reply":"2022-04-19T10:02:48.665344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_train_clf = images_train_clf.reshape(-1, 100, 100,1)\nimages_test_clf = images_test_clf.reshape(-1, 100, 100,1)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T09:27:25.87627Z","iopub.execute_input":"2022-04-19T09:27:25.876547Z","iopub.status.idle":"2022-04-19T09:27:25.881907Z","shell.execute_reply.started":"2022-04-19T09:27:25.876518Z","shell.execute_reply":"2022-04-19T09:27:25.880313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\n# preprocess data\nle = preprocessing.LabelEncoder()\nle.fit(labels_train_clf)\nlabels_train_clf = le.transform(labels_train_clf)\nlabels_test_clf = le.transform(labels_test_clf)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T09:39:33.431146Z","iopub.execute_input":"2022-04-19T09:39:33.431409Z","iopub.status.idle":"2022-04-19T09:39:33.439652Z","shell.execute_reply.started":"2022-04-19T09:39:33.431374Z","shell.execute_reply":"2022-04-19T09:39:33.438726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train and test classifier\nnum_classes = len(set(labels_train_clf))\nclassifier = create_classifier(encoder, trainable=False)\n\nhistory = classifier.fit(x=images_train_clf, y=labels_train_clf, batch_size=30, epochs=30)\naccuracy = classifier.evaluate(images_test_clf, labels_test_clf)[1]\nprint(f\"Test accuracy: {round(accuracy * 100, 2)}%\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:07:18.212469Z","iopub.execute_input":"2022-04-19T10:07:18.213053Z","iopub.status.idle":"2022-04-19T10:07:23.21004Z","shell.execute_reply.started":"2022-04-19T10:07:18.213016Z","shell.execute_reply":"2022-04-19T10:07:23.209181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Triplet loss","metadata":{}},{"cell_type":"code","source":"class SiameseNetwork(tf.keras.Model):\n    \"\"\"\n    A class that ceates the siamese network\n    ...\n    \n    Attributes\n    ----------\n    vgg_face: neural network architecture of VGG Face\n    \n    Methods\n    -------\n    call(inputs)\n        Return the VGG Face mappings of the anchor, the positive and the negative images\n    get_features(inputs)\n        Return the VGG Face mappings\n    \"\"\"\n    \n    def __init__(self, vgg_face):\n        \"\"\"\n        Class initialization\n        \n        param: vgg_face:  the model used for Siamese network\n        \"\"\"\n        \n        super(SiameseNetwork, self).__init__()\n        self.vgg_face = vgg_face\n        \n    @tf.function\n    def call(self, inputs):\n        \"\"\"\n        This function gives the VGG Face mappings of the anchor, the positive and the negative image\n    \n        param: inputs: list of the anchor, the positive and the negative images \n        :return: list of the embeddings of the anchor, the positive and the negative images\n        \"\"\"\n        \n        image_1, image_2, image_3 =  inputs\n        with tf.name_scope(\"Anchor\") as scope:\n            feature_1 = self.vgg_face(image_1)\n            feature_1 = tf.math.l2_normalize(feature_1, axis=-1)\n        with tf.name_scope(\"Positive\") as scope:\n            feature_2 = self.vgg_face(image_2)\n            feature_2 = tf.math.l2_normalize(feature_2, axis=-1)\n        with tf.name_scope(\"Negative\") as scope:\n            feature_3 = self.vgg_face(image_3)\n            feature_3 = tf.math.l2_normalize(feature_3, axis=-1)\n        return [feature_1, feature_2, feature_3]\n    \n    @tf.function\n    def get_features(self, inputs):\n        \"\"\"\n        VGG Face mappings \n\n        param: inputs: list of the anchor, the positive and the negative images\n        :return: list of l2 normalized embeddings of the anchor, the positive and the negative images\n        \"\"\"\n        return tf.math.l2_normalize(self.vgg_face(inputs, training=False), axis=-1)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T09:12:10.452384Z","iopub.execute_input":"2022-04-22T09:12:10.452798Z","iopub.status.idle":"2022-04-22T09:12:10.464558Z","shell.execute_reply.started":"2022-04-22T09:12:10.452753Z","shell.execute_reply":"2022-04-22T09:12:10.463631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K = tf.keras.backend\ndef loss_function(x, alpha = 0.2):\n    \"\"\"\n    Compute the loss function \n    \n    param: x: list of VGG Face embeddings of the anchor, the positive and the negative images\n    param: alpha: the fixed margin loss\n    :return: the value of the loss function\n    \"\"\"\n\n    # Triplet Loss function.\n    anchor,positive,negative = x\n    # distance between the anchor and the positive\n    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n    # distance between the anchor and the negative\n    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n    # compute loss\n    basic_loss = pos_dist-neg_dist+alpha\n    loss = K.mean(K.maximum(basic_loss,0.0))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-04-22T09:12:12.610453Z","iopub.execute_input":"2022-04-22T09:12:12.610868Z","iopub.status.idle":"2022-04-22T09:12:12.61718Z","shell.execute_reply.started":"2022-04-22T09:12:12.610834Z","shell.execute_reply":"2022-04-22T09:12:12.616082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n#binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()\ndef train(X):\n    \"\"\"\n    Compute the loss after applying \"Adam\" optimizer\n    \n    param: X: list of the anchor, the positive and the negative images\n    :return: the value of the loss function\n    \"\"\"\n\n    with tf.GradientTape() as tape:\n        y_pred = model(X)\n        loss = loss_function(y_pred, alpha=0.5)\n    grad = tape.gradient(loss, encoder.trainable_variables)\n    optimizer.apply_gradients(zip(grad, encoder.trainable_variables))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-04-22T09:12:13.271183Z","iopub.execute_input":"2022-04-22T09:12:13.271491Z","iopub.status.idle":"2022-04-22T09:12:13.290615Z","shell.execute_reply.started":"2022-04-22T09:12:13.271458Z","shell.execute_reply":"2022-04-22T09:12:13.28979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create siamese model\nencoder = create_encoder((100, 100, 1))\nmodel = SiameseNetwork(encoder)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T09:12:16.249701Z","iopub.execute_input":"2022-04-22T09:12:16.250166Z","iopub.status.idle":"2022-04-22T09:12:16.37376Z","shell.execute_reply.started":"2022-04-22T09:12:16.250125Z","shell.execute_reply":"2022-04-22T09:12:16.372976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set trainable layers\nfor layer in encoder.layers:\n    layer.trainable = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load our data in an online manner\ndata_generator = DataGenerator(dataset_path='../input/475-dataset/475_dataset/train/', batch_size=30)\n\n# Train the model\nlosses = []\naccuracy = []\nepochs = 400\nno_of_batches = data_generator.__len__()\nfor i in range(1, epochs+1, 1):\n    loss = 0\n    with tqdm(total=no_of_batches) as pbar:\n        \n        description = \"Epoch \" + str(i) + \"/\" + str(epochs)\n        pbar.set_description_str(description)\n        \n        for j in range(no_of_batches):\n            data = data_generator[j]\n            temp = train(data)\n            loss += temp\n            \n            pbar.update()\n            print_statement = \"Loss :\" + str(temp.numpy())\n            pbar.set_postfix_str(print_statement)\n        \n        loss /= no_of_batches\n        losses.append(loss.numpy())\n        # with file_writer.as_default():\n        #     tf.summary.scalar('Loss', data=loss.numpy(), step=i)\n            \n        print_statement = \"Loss :\" + str(loss.numpy())\n        \n        pbar.set_postfix_str(print_statement)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T10:00:34.213959Z","iopub.execute_input":"2022-04-22T10:00:34.214734Z","iopub.status.idle":"2022-04-22T10:39:59.208637Z","shell.execute_reply.started":"2022-04-22T10:00:34.214695Z","shell.execute_reply":"2022-04-22T10:39:59.207856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load our data in an online manner \ndata_generator = DataGenerator(dataset_path='../input/475-dataset/475_dataset/train/', batch_size=20)\n\n# Train the model\nlosses = []\naccuracy = []\nepochs = 800\nno_of_batches = data_generator.__len__()\nfor i in range(1, epochs+1, 1):\n    loss = 0\n    with tqdm(total=no_of_batches) as pbar:\n        \n        description = \"Epoch \" + str(i) + \"/\" + str(epochs)\n        pbar.set_description_str(description)\n        \n        for j in range(no_of_batches):\n            data = data_generator[j]\n            temp = train(data)\n            loss += temp\n            \n            pbar.update()\n            print_statement = \"Loss :\" + str(temp.numpy())\n            pbar.set_postfix_str(print_statement)\n        \n        loss /= no_of_batches\n        losses.append(loss.numpy())\n        # with file_writer.as_default():\n        #     tf.summary.scalar('Loss', data=loss.numpy(), step=i)\n            \n        print_statement = \"Loss :\" + str(loss.numpy())\n        \n        pbar.set_postfix_str(print_statement)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:50:48.861665Z","iopub.execute_input":"2022-04-19T14:50:48.861939Z","iopub.status.idle":"2022-04-19T15:11:57.898844Z","shell.execute_reply.started":"2022-04-19T14:50:48.861908Z","shell.execute_reply":"2022-04-19T15:11:57.897761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create classifier\nnum_classes = len(set(labels_train_clf))\nclassifier = create_classifier(encoder, trainable=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:48:32.420809Z","iopub.execute_input":"2022-04-19T14:48:32.421072Z","iopub.status.idle":"2022-04-19T14:48:32.499418Z","shell.execute_reply.started":"2022-04-19T14:48:32.421042Z","shell.execute_reply":"2022-04-19T14:48:32.498736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = classifier.fit(x=images_train_clf, y=labels_train_clf, batch_size=30, epochs=200)\n\naccuracy = classifier.evaluate(images_test_clf, labels_test_clf)[1]\nprint(f\"Test accuracy: {round(accuracy * 100, 2)}%\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:42:29.437336Z","iopub.execute_input":"2022-04-19T14:42:29.438132Z","iopub.status.idle":"2022-04-19T14:43:01.348381Z","shell.execute_reply.started":"2022-04-19T14:42:29.438096Z","shell.execute_reply":"2022-04-19T14:43:01.347722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"# get data\nimgs = images_train.reshape(-1, 100, 100,1)\nencoded_images = np.array(encoder(imgs))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T13:59:20.248019Z","iopub.execute_input":"2022-04-13T13:59:20.24854Z","iopub.status.idle":"2022-04-13T13:59:20.252472Z","shell.execute_reply.started":"2022-04-13T13:59:20.248505Z","shell.execute_reply":"2022-04-13T13:59:20.251525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n# plot 2D embeddings\npca = PCA(n_components=2)\nenc = pca.fit_transform(encoded_images)\nplt.scatter(enc[:, 0], enc[:, 1])","metadata":{"execution":{"iopub.status.busy":"2022-04-13T15:19:27.593145Z","iopub.execute_input":"2022-04-13T15:19:27.593389Z","iopub.status.idle":"2022-04-13T15:19:28.244343Z","shell.execute_reply.started":"2022-04-13T15:19:27.593361Z","shell.execute_reply":"2022-04-13T15:19:28.243592Z"},"trusted":true},"execution_count":null,"outputs":[]}]}