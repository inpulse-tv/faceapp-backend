{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import librairies","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport io\nimport cv2\nimport numpy as np\nfrom os import listdir\nfrom os.path import isfile, join\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\nimport keras\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\nfrom skimage.feature import hog\nfrom skimage import data, exposure\n\nimport tensorflow_addons as tfa\nimport random\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-04-16T22:36:37.402879Z","iopub.execute_input":"2022-04-16T22:36:37.40321Z","iopub.status.idle":"2022-04-16T22:36:43.841121Z","shell.execute_reply.started":"2022-04-16T22:36:37.403128Z","shell.execute_reply":"2022-04-16T22:36:43.840309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the FaceNet model","metadata":{}},{"cell_type":"code","source":"# example of loading the keras facenet model\nfrom keras.models import load_model\n# load the model\nmodel = load_model('../input/facenet-model/facenet_keras.h5')\n# summarize the model\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-16T22:36:45.830458Z","iopub.execute_input":"2022-04-16T22:36:45.83128Z","iopub.status.idle":"2022-04-16T22:36:52.66838Z","shell.execute_reply.started":"2022-04-16T22:36:45.831246Z","shell.execute_reply":"2022-04-16T22:36:52.667594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-processing funtions","metadata":{}},{"cell_type":"code","source":"def get_data(dataset_path):\n    \"\"\"\n    Extract images with their labels and names\n    \n    param: dataset_path: the path of the dataset\n    :return: a tuple of images, labels and names in an array format\n    \"\"\"\n    images = []\n    labels = []\n    names = []\n    try:\n        for dire in listdir(dataset_path): \n            for f in listdir(dataset_path+'/'+dire):\n                if \"jpeg\" in f or \"png\" in f or \"PNG\" in f or \"jpg\" in f:\n                    image_path = dataset_path + '/' + dire + '/' + f\n                    image = cv2.imread(image_path)\n                    image = np.asarray(image, dtype=np.float32)\n                    image = cv2.resize(image, (224, 224), interpolation = cv2.INTER_AREA)\n                    image = cv2.resize(image, (160, 160))\n                    image = image.astype('float32')\n                    mean, std = image.mean(), image.std()\n                    image = (image - mean) / std\n\n                    images.append(image)\n                    labels.append(dire)\n                    names.append(f)\n    except:\n         for f in listdir(dataset_path):\n                if \"jpeg\" in f or \"png\" in f or \"PNG\" in f or \"jpg\" in f:\n                    image_path = dataset_path + '/' + f\n                    image = cv2.imread(image_path)\n                    image = np.asarray(image, dtype=np.float32)\n                    image = cv2.resize(image, (224, 224), interpolation = cv2.INTER_AREA)\n                    image = cv2.resize(image, (160, 160))\n                    image = image.astype('float32')\n                    mean, std = image.mean(), image.std()\n                    image = (image - mean) / std\n\n                    images.append(image)\n                    labels.append(f)\n                    names.append(f)\n    return np.array(images), np.array(labels), np.array(names)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:27:48.896148Z","iopub.execute_input":"2022-04-06T08:27:48.896531Z","iopub.status.idle":"2022-04-06T08:27:48.914552Z","shell.execute_reply.started":"2022-04-06T08:27:48.896484Z","shell.execute_reply":"2022-04-06T08:27:48.91348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\ndef stats(labels):\n    \"\"\"\n    this fucntion gives the statistics of the dataset\n    param: labels: the images labels\n    :return: a list of different statistics of the dataset: number of labels, number of image, mean, standard deviation, minimum, maximum, quantile 50%.\n    \"\"\"\n    stat = defaultdict(int)\n    for l in labels:\n        stat[l] += 1\n    vals = list(stat.values())\n    n_labels, n_images = len(list(stat.keys())), sum(list(stat.values()))\n    m, sd = np.mean(vals), np.sqrt(np.var(vals))\n    mini, maxi = min(vals), max(vals)\n    q50, q90, q10 = np.quantile(vals, q=0.5), np.quantile(vals, q=0.9), np.quantile(vals, q=0.1)\n    \n    print(\"Number of labels : \",  len(list(stat.keys())))\n    print(\"Number of images : \",  sum(list(stat.values())))\n    print(\"Mean : \", m, \" ---- Standard deviation : \", sd)\n    print(\"Minimum : \", mini, \" ---- Maximum : \", maxi)\n    print(\"Quantile 50% : \", q50, \" ---- Quantile 90% : \", q90, \" ---- Quantile 10% : \", q10)\n    print(\"---------------------------------------------------------------------------------\")\n    return [n_labels, n_images, m, sd, mini, maxi, q50, q90, q10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy.linalg import norm\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.linear_model import SGDOneClassSVM\nfrom sklearn.covariance import EllipticEnvelope\ndef get_clean_data(labels, embeds, names, threshold=9, method='distance'):\n    \"\"\"\n    This function clean the data-set\n    :param labels: the images labels\n    :param embeds: the images embeddings\n    :param names: the file names of the images\n    :param threshold: according to this threshold we classify each image\n    :param method: the method used to clean the data\n    :return: a tuple of the outliers, cleaned embedding and labels, each one in an array format\n    \"\"\"\n    outliers =  {}\n    clean_embed = []\n    clean_labels = []\n    for k in set(labels):\n        \n        if method == 'distance':\n            filter_ = []\n            center = sum(embeds[k])/len(embeds[k])\n            for e in embeds[k]:\n                filter_.append(norm(e - center))\n            filter_ = np.array(filter_)\n            outliers[k] = np.array(names[k])[np.where(filter_ >= threshold)]\n            clean_embed.extend(np.array(embeds[k])[np.where(filter_ < threshold)])\n            n = len(np.array(embeds[k])[np.where(filter_ < threshold)])\n            \n        elif method == 'Gauss':\n            m = np.mean(embeds[k], axis=0)\n            v = np.var(embeds[k], axis=0)\n            filter_ = multivariateGaussian(embeds[k], m, v)\n            outliers[k] = np.array(names[k])[np.where(filter_ <= threshold)]\n            clean_embed.extend(np.array(embeds[k])[np.where(filter_ > threshold)])\n            n = len(np.array(embeds[k])[np.where(filter_ > threshold)])\n        \n        elif method == 'one_class_svm':\n            filter_ = SGDOneClassSVM(random_state=0).fit_predict(embeds[k])\n            outliers[k] = np.array(names[k])[np.where(filter_ == -1)]\n            clean_embed.extend(np.array(embeds[k])[np.where(filter_ == 1)])\n            n = len(np.array(embeds[k])[np.where(filter_ == 1)])\n        clean_labels.extend([k]*n)\n        #print(filter_, np.array(names[k]))\n\n    return outliers, np.array(clean_embed), np.array(clean_labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_embeddings(model, images):\n    \"\"\"\n    Get the embeddings of the images\n    \n    param: model: the embedding model (FaceNet)\n    :return: an array of embeddings\n    \"\"\"\n    embeddings = []\n    for img in images:\n        samples = np.expand_dims(img, axis=0)\n        yhat = model.predict(samples)\n        embeddings.append(yhat[0]) \n    return np.array(embeddings)    ","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:27:48.916971Z","iopub.execute_input":"2022-04-06T08:27:48.917616Z","iopub.status.idle":"2022-04-06T08:27:48.929523Z","shell.execute_reply.started":"2022-04-06T08:27:48.917568Z","shell.execute_reply":"2022-04-06T08:27:48.928054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def multivariateGaussian(X, mu, sigma):\n    \"\"\"\n    Compute the mulvariate gaussian distribution probability\n    \n    param: X: the input data\n    param: mu: the mean\n    param: sigma: the standard deviation\n    :return: the probability of X\n    \"\"\"\n    k = len(mu)\n    sigma=np.diag(sigma)\n    X = X - mu.T\n    prob = 1/((2*np.pi)**(k/2)*(np.linalg.det(sigma)**0.5))* np.exp(-0.5* np.sum(X @ np.linalg.pinv(sigma) * X,axis=1))\n    return prob","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:27:48.931032Z","iopub.execute_input":"2022-04-06T08:27:48.931318Z","iopub.status.idle":"2022-04-06T08:27:48.948131Z","shell.execute_reply.started":"2022-04-06T08:27:48.931284Z","shell.execute_reply":"2022-04-06T08:27:48.946946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn import preprocessing\n# le = preprocessing.LabelEncoder()\n# le.fit(labels_train)\n# labels_train = le.transform(labels_train)\n# labels_test = le.transform(labels_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:27:49.15651Z","iopub.execute_input":"2022-04-06T08:27:49.15688Z","iopub.status.idle":"2022-04-06T08:27:49.16035Z","shell.execute_reply.started":"2022-04-06T08:27:49.156847Z","shell.execute_reply":"2022-04-06T08:27:49.159598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from numpy import save\n# from numpy import load\n# save('embed_train.npy', embed_train)\n# data = load('embed_train.npy')","metadata":{"execution":{"iopub.status.busy":"2022-03-29T13:46:15.300394Z","iopub.execute_input":"2022-03-29T13:46:15.301114Z","iopub.status.idle":"2022-03-29T13:46:15.327009Z","shell.execute_reply.started":"2022-03-29T13:46:15.301075Z","shell.execute_reply":"2022-03-29T13:46:15.325983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n# from sklearn.svm import SVC\n# tuned_parameters = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']}\n\n# #clf = GridSearchCV(SVC(), tuned_parameters)\n# clf = SVC(C=10, gamma=0.001, kernel='rbf')\n# clf.fit(embed_train, labels_train)\n# #print(clf.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:27:54.443085Z","iopub.execute_input":"2022-03-29T09:27:54.443334Z","iopub.status.idle":"2022-03-29T09:27:56.088713Z","shell.execute_reply.started":"2022-03-29T09:27:54.443305Z","shell.execute_reply":"2022-03-29T09:27:56.087849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning data","metadata":{}},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"# example of Fernando Torres\nimages_person, _ , _ = get_data('../input/dataset7/photos1/Fernando Torres')\nembed_person = get_embeddings(model, images_person)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T11:51:42.26473Z","iopub.execute_input":"2022-04-01T11:51:42.265149Z","iopub.status.idle":"2022-04-01T11:51:51.107378Z","shell.execute_reply.started":"2022-04-01T11:51:42.265011Z","shell.execute_reply":"2022-04-01T11:51:51.106628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n# plot the different embeddings in 2D space\npca = PCA(n_components=2)\nembed_person_2d = pca.fit_transform(embed_person)\nplt.scatter(*zip(*embed_person_2d), marker = 'x')\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T11:51:51.108476Z","iopub.execute_input":"2022-04-01T11:51:51.108717Z","iopub.status.idle":"2022-04-01T11:51:51.351289Z","shell.execute_reply.started":"2022-04-01T11:51:51.108686Z","shell.execute_reply":"2022-04-01T11:51:51.350598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's remove outliers","metadata":{}},{"cell_type":"code","source":"# get the training and testing datasets\nimages_train2, labels_train2, names_train2 = get_data(\"../input/272-dataset/photos/Train/\")\nimages_test2, labels_test2, names_test2 = get_data(\"../input/272-dataset/photos/Test/\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T11:11:12.064022Z","iopub.execute_input":"2022-04-01T11:11:12.064694Z","iopub.status.idle":"2022-04-01T11:11:59.074928Z","shell.execute_reply.started":"2022-04-01T11:11:12.064656Z","shell.execute_reply":"2022-04-01T11:11:59.074155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute embeddings\nembed_train2 = get_embeddings(model, images_train2)\nembed_test2 = get_embeddings(model, images_test2)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T11:02:12.197924Z","iopub.execute_input":"2022-04-01T11:02:12.198187Z","iopub.status.idle":"2022-04-01T11:07:56.200775Z","shell.execute_reply.started":"2022-04-01T11:02:12.198153Z","shell.execute_reply":"2022-04-01T11:07:56.200006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import save\nfrom numpy import load\n\n# save embeddings to avoid recomputing them\nsave('embed_train2.npy', embed_train2)\nsave('embed_test2.npy', embed_test2)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T11:07:56.202919Z","iopub.execute_input":"2022-04-01T11:07:56.203158Z","iopub.status.idle":"2022-04-01T11:07:56.212203Z","shell.execute_reply.started":"2022-04-01T11:07:56.203121Z","shell.execute_reply":"2022-04-01T11:07:56.211416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the training and testing datasets (another one)\nimages_train, labels_train, names_train = get_data(\"../input/475-dataset/475_dataset/train/\")\nimages_test, labels_test, names_test = get_data(\"../input/475-dataset/475_dataset/test/\")","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:27:49.162164Z","iopub.execute_input":"2022-04-06T08:27:49.162765Z","iopub.status.idle":"2022-04-06T08:30:11.729148Z","shell.execute_reply.started":"2022-04-06T08:27:49.16273Z","shell.execute_reply":"2022-04-06T08:30:11.727903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the statistics of this dataset\nst_train = stats(labels_train)\nst_test = stats(labels_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:30:11.733172Z","iopub.execute_input":"2022-04-06T08:30:11.73401Z","iopub.status.idle":"2022-04-06T08:30:11.762368Z","shell.execute_reply.started":"2022-04-06T08:30:11.733951Z","shell.execute_reply":"2022-04-06T08:30:11.761544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute embeddings\nembed_train = get_embeddings(model, images_train)\nembed_test = get_embeddings(model, images_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:43:31.048598Z","iopub.execute_input":"2022-04-01T10:43:31.048839Z","iopub.status.idle":"2022-04-01T10:52:53.327165Z","shell.execute_reply.started":"2022-04-01T10:43:31.048807Z","shell.execute_reply":"2022-04-01T10:52:53.326471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import save\n\n# save embeddings\nsave('embed_train.npy', embed_train)\nsave('embed_test.npy', embed_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import load\n\n# load the different embeddings\nembed_test1 = load('../input/embeddings/embed_test.npy')\nembed_train2 = load('../input/embeddings2/embed_train2.npy')\nembed_test2 = load('../input/embeddings2/embed_test2.npy')","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:30:11.763824Z","iopub.execute_input":"2022-04-06T08:30:11.764128Z","iopub.status.idle":"2022-04-06T08:30:12.038448Z","shell.execute_reply.started":"2022-04-06T08:30:11.764095Z","shell.execute_reply":"2022-04-06T08:30:12.03749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_train = embed_train1\nembed_test = embed_test1","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:30:12.040973Z","iopub.execute_input":"2022-04-06T08:30:12.041213Z","iopub.status.idle":"2022-04-06T08:30:12.04603Z","shell.execute_reply.started":"2022-04-06T08:30:12.041185Z","shell.execute_reply":"2022-04-06T08:30:12.045056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenate the two datasets \nimages_train = np.concatenate((images_train1, images_train2), axis=0)\nlabels_train = np.concatenate((labels_train1, labels_train2), axis=0)\nnames_train = np.concatenate((names_train1, names_train2), axis=0)\nembed_train = np.concatenate((embed_train1, embed_train2), axis=0)\n\nimages_test = np.concatenate((images_test1, images_test2), axis=0)\nlabels_test = np.concatenate((labels_test1, labels_test2), axis=0)\nnames_test = np.concatenate((names_test1, names_test2), axis=0)\nembed_test = np.concatenate((embed_test1, embed_test2), axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T11:13:15.442538Z","iopub.execute_input":"2022-04-01T11:13:15.442802Z","iopub.status.idle":"2022-04-01T11:13:20.392194Z","shell.execute_reply.started":"2022-04-01T11:13:15.442766Z","shell.execute_reply":"2022-04-01T11:13:20.386936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# Warning !! Use this cell only if you want to use the gaussian mixture method as your cleaning method\npca = PCA(0.9)\nembed_train = pca.fit_transform(embed_train)\nembed_test = pca.transform(embed_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:30:12.047649Z","iopub.execute_input":"2022-04-06T08:30:12.047916Z","iopub.status.idle":"2022-04-06T08:30:12.145866Z","shell.execute_reply.started":"2022-04-06T08:30:12.047886Z","shell.execute_reply":"2022-04-06T08:30:12.144449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\n# create dictionaries of embeddings and names to pass them as arguments to the cleaning function\nembeds_train_dict = defaultdict(list)\nnames_train_dict = defaultdict(list)\nfor i, k in enumerate(labels_train):\n    embeds_train_dict[k].append(embed_train[i])\n    names_train_dict[k].append(names_train[i])\n\nnames_test_dict = defaultdict(list)\nembeds_test_dict = defaultdict(list)\nfor i, k in enumerate(labels_test):\n    embeds_test_dict[k].append(embed_test[i])\n    names_test_dict[k].append(names_test[i])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T10:52:53.425167Z","iopub.execute_input":"2022-04-08T10:52:53.425523Z","iopub.status.idle":"2022-04-08T10:52:53.504119Z","shell.execute_reply.started":"2022-04-08T10:52:53.425443Z","shell.execute_reply":"2022-04-08T10:52:53.503185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cleaning data\noutliers_train, clean_embed_train, clean_labels_train = get_clean_data(labels_train, embeds_train_dict, names_train_dict)\noutliers_test, clean_embed_test, clean_labels_test = get_clean_data(labels_test, embeds_test_dict, names_test_dict)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:30:12.270651Z","iopub.execute_input":"2022-04-06T08:30:12.271122Z","iopub.status.idle":"2022-04-06T08:30:12.473348Z","shell.execute_reply.started":"2022-04-06T08:30:12.27109Z","shell.execute_reply":"2022-04-06T08:30:12.47237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stats of the cleaned data\nclean_st_train = stats(clean_labels_train)\nclean_st_test = stats(clean_labels_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:30:12.474972Z","iopub.execute_input":"2022-04-06T08:30:12.475197Z","iopub.status.idle":"2022-04-06T08:30:12.497125Z","shell.execute_reply.started":"2022-04-06T08:30:12.47517Z","shell.execute_reply":"2022-04-06T08:30:12.494438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the number of the removed labels and images\nprint(\"Train labels: \", 475 - clean_st_train[0])\nprint(\"Train images: \", 7805 - clean_st_train[1])\nprint(\"Test labels: \", 467 - clean_st_test[0])\nprint(\"Test images: \", 3363 - clean_st_test[1])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T08:30:12.498551Z","iopub.execute_input":"2022-04-06T08:30:12.498877Z","iopub.status.idle":"2022-04-06T08:30:12.506857Z","shell.execute_reply.started":"2022-04-06T08:30:12.498825Z","shell.execute_reply":"2022-04-06T08:30:12.505966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\n\n# shuffle data \nembed_train, labels_train = shuffle(embed_train, labels_train)\nembed_test, labels_test = shuffle(embed_test, labels_test)\n\nclean_embed_train, clean_labels_train = shuffle(clean_embed_train, clean_labels_train)\nclean_embed_test, clean_labels_test = shuffle(clean_embed_test, clean_labels_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T12:51:38.205491Z","iopub.execute_input":"2022-04-04T12:51:38.205762Z","iopub.status.idle":"2022-04-04T12:51:38.218212Z","shell.execute_reply.started":"2022-04-04T12:51:38.205735Z","shell.execute_reply":"2022-04-04T12:51:38.217423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\n# train SVM classifier on uncleaned data\nclf = SVC(C=10, gamma=0.001, kernel='rbf')\nclf.fit(embed_train, labels_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T11:14:19.461632Z","iopub.execute_input":"2022-04-01T11:14:19.462035Z","iopub.status.idle":"2022-04-01T11:14:36.602904Z","shell.execute_reply.started":"2022-04-01T11:14:19.462001Z","shell.execute_reply":"2022-04-01T11:14:36.602213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# accuracy of the SVM classifier on uncleaned data\npreds = clf.predict(embed_test)\nprint(\"The accuracy of the model is :\", accuracy_score(preds, labels_test)*100, \"%\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T11:14:36.605511Z","iopub.execute_input":"2022-04-01T11:14:36.605715Z","iopub.status.idle":"2022-04-01T11:16:29.586807Z","shell.execute_reply.started":"2022-04-01T11:14:36.60569Z","shell.execute_reply":"2022-04-01T11:16:29.586036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\n\n# train SVM classifier on cleaned data\nclf = SVC(C=10, gamma=0.001, kernel='rbf')\nclf.fit(clean_embed_train, clean_labels_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T12:53:38.008707Z","iopub.execute_input":"2022-04-04T12:53:38.008992Z","iopub.status.idle":"2022-04-04T12:53:44.834989Z","shell.execute_reply.started":"2022-04-04T12:53:38.008962Z","shell.execute_reply":"2022-04-04T12:53:44.834021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# accuracy of the SVM classifier on cleaned data\npreds = clf.predict(clean_embed_test)\nprint(\"The accuracy of the model is :\", accuracy_score(preds, clean_labels_test)*100, \"%\")","metadata":{"execution":{"iopub.status.busy":"2022-04-04T12:53:44.837162Z","iopub.execute_input":"2022-04-04T12:53:44.837507Z","iopub.status.idle":"2022-04-04T12:54:14.696189Z","shell.execute_reply.started":"2022-04-04T12:53:44.837463Z","shell.execute_reply":"2022-04-04T12:54:14.695105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Network Classifier","metadata":{}},{"cell_type":"code","source":"# number of classes\nn_classes = len(set(clean_labels_train))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:52:54.27914Z","iopub.execute_input":"2022-04-01T10:52:54.279372Z","iopub.status.idle":"2022-04-01T10:52:54.287501Z","shell.execute_reply.started":"2022-04-01T10:52:54.279339Z","shell.execute_reply":"2022-04-01T10:52:54.286416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\n\n# preprocess data\nle = preprocessing.LabelEncoder()\nle.fit(clean_labels_train)\nclf_clean_labels_train = le.transform(clean_labels_train)\nclf_clean_labels_test = le.transform(clean_labels_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:52:54.289065Z","iopub.execute_input":"2022-04-01T10:52:54.289339Z","iopub.status.idle":"2022-04-01T10:52:54.309001Z","shell.execute_reply.started":"2022-04-01T10:52:54.289303Z","shell.execute_reply":"2022-04-01T10:52:54.308318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define neural network classifier\ndef classifier():\n    clf = keras.Sequential()\n    clf.add(layers.Dense(1024//2, activation='relu', input_dim=128))\n    clf.add(layers.Dropout(0.5))\n    clf.add(layers.Dense(1024//4, activation='relu'))\n    clf.add(layers.Dropout(0.5)) \n    # Number of classes !!!\n    clf.add(Dense(n_classes, activation='softmax'))\n    return clf\nclf2 = classifier()\nclf2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:52:54.659149Z","iopub.execute_input":"2022-04-01T10:52:54.659485Z","iopub.status.idle":"2022-04-01T10:52:54.697354Z","shell.execute_reply.started":"2022-04-01T10:52:54.659433Z","shell.execute_reply":"2022-04-01T10:52:54.696696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fitting the model\nclf2.compile(optimizer=tf.keras.optimizers.Adam(lr=0.00006), loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\nclf_hist = clf2.fit(clean_embed_train, clf_clean_labels_train, validation_split=0.2,epochs = 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:52:54.700033Z","iopub.execute_input":"2022-04-01T10:52:54.700219Z","iopub.status.idle":"2022-04-01T10:54:17.009496Z","shell.execute_reply.started":"2022-04-01T10:52:54.700197Z","shell.execute_reply":"2022-04-01T10:54:17.0086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n# evaluating the model\nclf2.evaluate(clean_embed_test, clf_clean_labels_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:54:17.011702Z","iopub.execute_input":"2022-04-01T10:54:17.011972Z","iopub.status.idle":"2022-04-01T10:54:17.363039Z","shell.execute_reply.started":"2022-04-01T10:54:17.011934Z","shell.execute_reply":"2022-04-01T10:54:17.362351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA 2dim","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# reduce the embedding dimentionality to 2D dimensions\npca = PCA(n_components = 2)\nembed_train_2d = pca.fit_transform(clean_embed_train)\nembed_test_2d = pca.transform(clean_embed_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:21:38.950081Z","iopub.execute_input":"2022-03-31T08:21:38.950294Z","iopub.status.idle":"2022-03-31T08:21:38.988575Z","shell.execute_reply.started":"2022-03-31T08:21:38.950268Z","shell.execute_reply":"2022-03-31T08:21:38.987558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\n\n# train the SVC classifier\nclf = SVC(C=10, gamma=0.001, kernel='rbf')\nclf.fit(embed_train_2d, clean_labels_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:21:38.991553Z","iopub.execute_input":"2022-03-31T08:21:38.996766Z","iopub.status.idle":"2022-03-31T08:21:39.895911Z","shell.execute_reply.started":"2022-03-31T08:21:38.996694Z","shell.execute_reply":"2022-03-31T08:21:39.894937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# predictions accuracy\npreds = clf.predict(embed_test_2d)\nprint(\"The accuracy of the model is :\", accuracy_score(preds, clean_labels_test)*100, \"%\")","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:21:39.897347Z","iopub.execute_input":"2022-03-31T08:21:39.897664Z","iopub.status.idle":"2022-03-31T08:21:40.701238Z","shell.execute_reply.started":"2022-03-31T08:21:39.897623Z","shell.execute_reply":"2022-03-31T08:21:40.700226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA 0.9 variance","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n# try with 0.9 variance\npca = PCA(0.9)\nembed_train = pca.fit_transform(clean_embed_train)\nembed_test = pca.transform(clean_embed_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:21:40.702787Z","iopub.execute_input":"2022-03-31T08:21:40.703115Z","iopub.status.idle":"2022-03-31T08:21:40.737104Z","shell.execute_reply.started":"2022-03-31T08:21:40.703059Z","shell.execute_reply":"2022-03-31T08:21:40.736132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"number of components is : \", pca.n_components_)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:21:40.738536Z","iopub.execute_input":"2022-03-31T08:21:40.739069Z","iopub.status.idle":"2022-03-31T08:21:40.746714Z","shell.execute_reply.started":"2022-03-31T08:21:40.739016Z","shell.execute_reply":"2022-03-31T08:21:40.745479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\n# train classifier\nclf = SVC(C=10, gamma=0.001, kernel='rbf')\nclf.fit(embed_train, clean_labels_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:21:40.749162Z","iopub.execute_input":"2022-03-31T08:21:40.750389Z","iopub.status.idle":"2022-03-31T08:21:41.914341Z","shell.execute_reply.started":"2022-03-31T08:21:40.750329Z","shell.execute_reply":"2022-03-31T08:21:41.913265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n# accuracy\npreds = clf.predict(embed_test)\nprint(\"The accuracy of the model is :\", accuracy_score(preds, clean_labels_test)*100, \"%\")","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:21:41.915409Z","iopub.execute_input":"2022-03-31T08:21:41.915633Z","iopub.status.idle":"2022-03-31T08:21:42.839826Z","shell.execute_reply.started":"2022-03-31T08:21:41.915605Z","shell.execute_reply":"2022-03-31T08:21:42.839018Z"},"trusted":true},"execution_count":null,"outputs":[]}]}